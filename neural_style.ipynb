{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4f87b3-6a0a-49db-8fbb-4faa9f9e0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85358df4-b163-446a-bb87-bb218c9488fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Images\n",
    "def load_image(image_path, max_size=400):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Resize the image\n",
    "    size = max_size if max(image.size) > max_size else max(image.size)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    image = transform(image)[:3, :, :].unsqueeze(0)\n",
    "    return image.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "620bb0ec-b32c-42e2-85f8-f897e3a53735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Image\n",
    "def show_image(tensor):\n",
    "    image = tensor.clone().detach().cpu().squeeze(0)\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315f6da8-a671-4801-9f1e-f4e91f7708e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained VGG19 Model\n",
    "class VGG19Features(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19Features, self).__init__()\n",
    "        self.model = models.vgg19(pretrained=True).features[:29].eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = {}\n",
    "        layers = {\n",
    "            '1': 'conv1_1', '6': 'conv2_1', '11': 'conv3_1', '20': 'conv4_1', '29': 'conv5_1'\n",
    "        }\n",
    "        \n",
    "        for name, layer in self.model._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in layers:\n",
    "                features[layers[name]] = x\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e230ee39-90f4-4842-80c3-5c42138506a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Content Loss\n",
    "def content_loss(target_features, generated_features):\n",
    "    return torch.mean((generated_features - target_features) ** 2)\n",
    "\n",
    "# Compute Style Loss\n",
    "def gram_matrix(tensor):\n",
    "    _, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    return torch.mm(tensor, tensor.t())\n",
    "\n",
    "def style_loss(style_features, generated_features):\n",
    "    loss = 0\n",
    "    for layer in style_features:\n",
    "        gram_s = gram_matrix(style_features[layer])\n",
    "        gram_g = gram_matrix(generated_features[layer])\n",
    "        loss += torch.mean((gram_s - gram_g) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901acf37-a5be-4cae-a80a-f7ab1972e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Style Transfer\n",
    "def neural_style_transfer(content_path, style_path, steps=500, alpha=1e4, beta=1e8):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    content = load_image(content_path).to(device)\n",
    "    style = load_image(style_path).to(device)\n",
    "    \n",
    "    model = VGG19Features().to(device)\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "    optimizer = optim.Adam([target], lr=0.003)\n",
    "    \n",
    "    content_features = model(content)\n",
    "    style_features = model(style)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        generated_features = model(target)\n",
    "        c_loss = content_loss(content_features['conv4_1'], generated_features['conv4_1'])\n",
    "        s_loss = style_loss(style_features, generated_features)\n",
    "        total_loss = alpha * c_loss + beta * s_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{steps}, Loss: {total_loss.item():.4f}\")\n",
    "    \n",
    "    show_image(target)\n",
    "    return target\n",
    "\n",
    "# Example Usage\n",
    "styled_image = neural_style_transfer(r\"C:\\Users\\poojan\\OneDrive\\Desktop\\CODETECH_IT_SOLUTION\\sample1.jpg\", r\"C:\\Users\\poojan\\OneDrive\\Desktop\\CODETECH_IT_SOLUTION\\sample2.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
